{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID-19_detector.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i76U0XcqBbX"
      },
      "source": [
        "# COVID-19 Detector\n",
        "\n",
        "This file is a COVID-19 Detector. Previously to this file we have done a dataset_creator in Jupyter Notebook. Firsly, we upload the folders of train and test obtained with the dataset_creator to drive. Then, we start working."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJsJFiv333ap"
      },
      "source": [
        "Date: 2021-03-02\n",
        "________________________________________________________________________________\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=nHQDDAAzIsI\n",
        "\n",
        "Covid X-Ray Image Dataset for positive cases - https://github.com/ieee8023/covid-che...​ \n",
        "\n",
        "Kaggle X-Ray Chest Images for negative cases - https://www.kaggle.com/paultimothymoo...​ \n",
        "\n",
        "________________________________________________________________________________\n",
        "\n",
        "**IMPORTANT:** To run this notebook it is necessary to change the runtime type to GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFUBKj4DJe5X"
      },
      "source": [
        "# Import libraries\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import keras\r\n",
        "from keras.layers import *\r\n",
        "from keras.models import *\r\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a8PBa2cJZSP"
      },
      "source": [
        "First, we specify the path to the training and test folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_mSi5fMI8K3"
      },
      "source": [
        "TRAIN_PATH = \"/content/drive/MyDrive/TFM_workspace/video_coding_blocks/Dataset/train\"\r\n",
        "TEST_PATH = \"/content/drive/MyDrive/TFM_workspace/video_coding_blocks/Dataset/test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kkg_UGNJUGn"
      },
      "source": [
        "The images are very large, so we have to resize them.\r\n",
        "\r\n",
        "Let's try to build a CNN based model in Keras. First we are going to create multiple 3 o 4 CNN layers followed by a classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrPN31LQKVGY"
      },
      "source": [
        "# CNN Based Model in Keras\r\n",
        "\r\n",
        "# 4 Convolutional layers\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32,kernel_size=(3,3), activation=\"relu\", input_shape = (224,224,3)))\r\n",
        "model.add(Conv2D(64,(3,3), activation=\"relu\"))\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25)) # to find overfitting\r\n",
        "\r\n",
        "model.add(Conv2D(64,(3,3), activation=\"relu\"))\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "model.add(Conv2D(128,(3,3), activation=\"relu\"))\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "# Dense layers\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(64,activation=\"relu\"))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\r\n",
        "\r\n",
        "model.compile(loss=keras.losses.binary_crossentropy, optimizer = \"adam\",\r\n",
        "              metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TxGMxfrO46R"
      },
      "source": [
        "Firstly we will put our angulation layer with 32 number of filters. We are keeping the number of filters small in the beginning, because the lower layers detect features in very small part of the image. If you only see a very small part of an image, there are a very few patterns like dots, textures that you can make.\r\n",
        "\r\n",
        "As you go deeper into the network, your receptive field of the CNN layer increases. That means the feature that it extracts is basically based on a bigger part in the original picture.\r\n",
        "\r\n",
        "In this case we will have 32 different type of features we will extract in the first layer.\r\n",
        "\r\n",
        "Kernel sizes: 3x3. It's a standard choice. \r\n",
        "\r\n",
        "We will use the REU activation.\r\n",
        "\r\n",
        "Since this is the first layer, we need to specify the input shape, so we can specify the shape as (224,224,3).\r\n",
        "\r\n",
        "If you add Max Pooling it is going to further increase your receptive field, so you are looking at a bigger layer.\r\n",
        "\r\n",
        "We are not using the 5x5 filters directly, because adding two layers of 3x3 will increase the non-linearity. This will increase the non-linearity in the network and that will increase the hypothesis that we can represent more complex functions with more non-linear functions. Another reason is to reduce the number of parameters.\r\n",
        "\r\n",
        "As I go deeper in the network, I will also increase the number of filters.\r\n",
        "\r\n",
        "In the final layer we put a single neuron because we are performing a binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iETz_-hIfxHw",
        "outputId": "9ba7b0c8-b567-4ebd-cc30-642a527650ea"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 220, 220, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 110, 110, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 110, 110, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 108, 108, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 52, 52, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 86528)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                5537856   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,668,097\n",
            "Trainable params: 5,668,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57VHBaj_iP8y"
      },
      "source": [
        "So we have 5,668,097 parameters in this model and we are going to train it from scratch.\r\n",
        "\r\n",
        "One trend that you need to notice is that as you go deeper into the model, you need to increase the number of channels.\r\n",
        "\r\n",
        "For the training we will use the Keras image data generator library to make the data ready for the model:\r\n",
        "\r\n",
        "* We will rescale the data by factor 1/255. This will help us to the normalization.\r\n",
        "\r\n",
        "* Zoom allow us to take some randon crops of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPppULfIibKp"
      },
      "source": [
        "# Train from scratch\r\n",
        "\r\n",
        "train_datagen = image.ImageDataGenerator(\r\n",
        "    rescale = 1./255,\r\n",
        "    shear_range = 0.2,\r\n",
        "    zoom_range = 0.2,\r\n",
        "    horizontal_flip = True\r\n",
        ")\r\n",
        "\r\n",
        "# In the test we also do the resize\r\n",
        "test_dataset = image.ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcFOCTPDkFDb",
        "outputId": "c93094bd-42e0-43c4-940e-777cffa06832"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    \"/content/drive/MyDrive/TFM_workspace/video_coding_blocks/Dataset/train\",\r\n",
        "    target_size = (224,224),\r\n",
        "    batch_size = 32,\r\n",
        "    class_mode = \"binary\"\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 294 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3W5wSxzks4L",
        "outputId": "42ba39e2-60ca-4bdc-d8f5-f503dd1d3405"
      },
      "source": [
        "# What casses?\r\n",
        "train_generator.class_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Covid': 0, 'Normal': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX0EAvqZl0Hz"
      },
      "source": [
        "We have to do the same thing for validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf-h31oRljKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465a18c7-0615-4528-97bb-7584b67e83be"
      },
      "source": [
        "validation_generator = test_dataset.flow_from_directory(\r\n",
        "    \"/content/drive/MyDrive/TFM_workspace/video_coding_blocks/Dataset/test\",\r\n",
        "    target_size = (224,224),\r\n",
        "    batch_size = 32,\r\n",
        "    class_mode = \"binary\"\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 98 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjrn1pbcmLek",
        "outputId": "6d3b399a-4036-460e-e79a-be7676e07710"
      },
      "source": [
        "validation_generator.class_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Covid': 0, 'Normal': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BwyCgS3osMR",
        "outputId": "75feb03b-b016-4c0c-9133-641a4786fdeb"
      },
      "source": [
        "hist = model.fit_generator(\r\n",
        "    train_generator,\r\n",
        "    steps_per_epoch = 8,\r\n",
        "    epochs = 10,\r\n",
        "    validation_data = validation_generator,\r\n",
        "    validation_steps = 2\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 175s 18s/step - loss: 0.9217 - accuracy: 0.5093 - val_loss: 0.6916 - val_accuracy: 0.4375\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 22s 3s/step - loss: 0.6690 - accuracy: 0.6663 - val_loss: 0.5918 - val_accuracy: 0.5469\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.5522 - accuracy: 0.7216 - val_loss: 0.4290 - val_accuracy: 0.9531\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3991 - accuracy: 0.8183 - val_loss: 0.3633 - val_accuracy: 0.9219\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3550 - accuracy: 0.8786 - val_loss: 0.1937 - val_accuracy: 0.9219\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4196 - accuracy: 0.8040 - val_loss: 0.2367 - val_accuracy: 0.9062\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.3961 - accuracy: 0.8727 - val_loss: 0.2909 - val_accuracy: 0.9688\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2701 - accuracy: 0.9304 - val_loss: 0.1967 - val_accuracy: 0.9531\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.1797 - accuracy: 0.9297 - val_loss: 0.1334 - val_accuracy: 0.9531\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2765 - accuracy: 0.8824 - val_loss: 0.1564 - val_accuracy: 0.9531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZYgjhPqHrS"
      },
      "source": [
        "We have obtained an accuracy of 95.31% and a loss of 15.64% in 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFOU9no2r03i"
      },
      "source": [
        "## FALTA CONFUSION MATRIX"
      ]
    }
  ]
}